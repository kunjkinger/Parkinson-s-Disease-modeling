{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1b035ff-e0a8-4d7a-bdd6-ee7b9eb81a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio import SeqIO\n",
    "from collections import Counter\n",
    "from Bio.PDB import PDBParser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score, recall_score,classification_report,confusion_matrix\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import json\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import nglview as nv\n",
    "import os\n",
    "import sklearn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8c5fc08c-c1a4-405a-84cc-04eb9fa2e6dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\kunjk\\kunj\\lib\\site-packages)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\kunjk\\kunj\\lib\\site-packages (4.9.2)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: huggingface-hub==0.0.12 in c:\\users\\kunjk\\kunj\\lib\\site-packages (from transformers) (0.0.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\kunjk\\kunj\\lib\\site-packages)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kunjk\\kunj\\lib\\site-packages (from transformers) (1.19.5)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\kunjk\\kunj\\lib\\site-packages)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in c:\\users\\kunjk\\kunj\\lib\\site-packages (from transformers) (2.25.1)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -cipy (c:\\users\\kunjk\\kunj\\lib\\site-packages)"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in c:\\users\\kunjk\\kunj\\lib\\site-packages (from transformers) (0.0.45)\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\kunjk\\kunj\\lib\\site-packages (from transformers) (3.10.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\kunjk\\kunj\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kunjk\\kunj\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in c:\\users\\kunjk\\kunj\\lib\\site-packages (from transformers) (0.10.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kunjk\\kunj\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\kunjk\\kunj\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kunjk\\kunj\\lib\\site-packages (from transformers) (4.62.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kunjk\\kunj\\lib\\site-packages (from huggingface-hub==0.0.12->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\kunjk\\kunj\\lib\\site-packages (from packaging->transformers) (2.4.7)"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f0aa6a8-b9fd-4993-a560-beee1c70b5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import BertTokenizer, get_linear_schedule_with_warmup\n",
    "import torch_optimizer as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fee0d378-c60d-4d10-8654-e52f6be0622d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_def import ProteinClassifier\n",
    "from data_prep import ProteinSequenceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "161f0698-3770-437e-9aee-b1c0bf38961d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17a13dd246e94ef6b2674237b2f1c2e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/81.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b74c259c4f243bea4554a593e907d19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "131970e4c7e340099a2e6ee5680f9868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/210 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3356badae3cd4f12b99be5eac9192a87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_len = 100 # providing the maximum length of a sequence\n",
    "PRE_TRAINED_MODEL_NAME = 'Rostlab/prot_bert_bfd_localization'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME, do_lower_case = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1cc5ffa3-d5f5-4d8b-86c0-4796bacb091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_train_data_loader(batch_size,training_dir):\n",
    "    dataset = pd.read_csv(os.path.join(training_dir,'train.csv'))\n",
    "    train_data = ProteinSequenceDataset(sequence=dataset.sequence.to_numpy(),\n",
    "                                        targets=dataset.gene.to_numpy(),\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        max_len=MAX_LEN)\n",
    "    \n",
    "    train_sampler = torch.utils.data.DistributedSampler(dataset,\n",
    "                                                       num_replicas=dist.get_world_size(),\n",
    "                                                       rank=dist.get_rank())\n",
    "    train_dataloader = DataLoader(train_data,batch_size=batch_size,shuffle=True,num_workers=0,pin_memory=True,\n",
    "                                 sampler=train_sampler)\n",
    "    return train_dataloader\n",
    "\n",
    "def _get_test_data_loader(batch_size,training_dir):\n",
    "    dataset = pd.read_csv(os.path.join(training_dir,'test.csv'))\n",
    "    test_data = ProteinSequenceDataset(sequence=dataset.sequence.to_numpy(),\n",
    "                                        targets=dataset.ggene.to_numpy(),\n",
    "                                        tokenizer=tokenizer,\n",
    "                                        max_len=MAX_LEN)\n",
    "    \n",
    "    test_sampler = RandomSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data,sampler=test_sampler,batch_size=batch_size)\n",
    "    return test_dataloader\n",
    "                                       \n",
    "def freeze(model,frozen_layers):\n",
    "    modules = [model.bert.encoder.layer[:frozen_layers]]\n",
    "    for module in modules:\n",
    "        for param in module.parameters():\n",
    "            param.requires_grad=False\n",
    "                                       \n",
    "def train(args):\n",
    "    use_cuda=args.num_gpus > 0\n",
    "    device= torch.device('cuda' if use_cuda else 'cpu')\n",
    "    world_size= dist.get_world_size()\n",
    "    rank = dist.get_rank()\n",
    "    local_rank=dist.get_local_rank()\n",
    "    \n",
    "    #set the random seed for generating random numbers\n",
    "    torch.manual_seed(args.seed)\n",
    "    is use_cuda:\n",
    "        torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "    train_loader = _get_train_data_loader(args.batch_size,args.data_dir)\n",
    "    \n",
    "    if rank==0:\n",
    "        test_loader = _get_test_data_loader(args.test_batch_size,args.test)\n",
    "        print('max length of sequence: ',MAX_LEN)\n",
    "        print('freezing {} layers'.format(args.frozen_layers))\n",
    "        print('Model used: ',PRE_TRAINED_MODEL_NAME)\n",
    "        \n",
    "    logger.debug(\n",
    "        'Prcosses {}/{} ({.0f}%) of train data'.format(len(train_loader.sampler),\n",
    "                                                      len(train_loader.dataset),\n",
    "                                                      100.0 = len(train_loader.sampler)/len(train_loader.dataset),\n",
    "                                                      )\n",
    "    )\n",
    "    model = ProteinClassifier(args.num_labels) # the number of output labels\n",
    "    freeze(model,args.frozen_layers)\n",
    "    model = DDP(model.to(device),broadcast_buffers=False)\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    model.cuda(local_rank)\n",
    "    \n",
    "    optimizer = optim.Lamb(model.parameters(),\n",
    "                          lr = args.lr * dist.get_world_size(),\n",
    "                          betas = (0.9,0.999),\n",
    "                          eps=args.epsilon,\n",
    "                          weight_decay=args.weight_dacay)\n",
    "    total_steps=len(train_loader.dataset)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                               num_warmup_steps=0,\n",
    "                                               num_training_steps=total_steps)\n",
    "    \n",
    "    loss_fn == nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    for epoch in enage(1,args.epochs+1):\n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_loader):\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            b_input_mask = batch['attention_mask'].to(device)\n",
    "            b_labels = batch['targets'].to(device)\n",
    "            \n",
    "            outputs = model(b_input_ids.attention_mask=b_input_mask)\n",
    "            loss = loss_fn(outputs,b_labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(),1.0)\n",
    "            #modified based on their gradients, the learning rate etc\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if step% args.log_interval == 0 and rank == 0:\n",
    "                logger.info(\n",
    "                'collections data from master Node: \\n Train Epoch: {} [{}/{} ({:.0f}%)] Training Loss: {:.6f}'.format(\n",
    "                    epoch,\n",
    "                step * len(batch['input_ids'])%world_size,\n",
    "                len(train_loader.dataset),\n",
    "                100.0 *step/len(train_loader),\n",
    "                loss.item(),\n",
    "                ))\n",
    "            \n",
    "            if args.verbose:\n",
    "                print('Batch',step,'from rank',rank)\n",
    "        if rank == 0:\n",
    "            test(model,test_loader,device)\n",
    "        scheduler.step()\n",
    "    if rank == 0:\n",
    "        model_save = model.module if hasattr(model,'module') else model\n",
    "        save_model(model_save,args.model_dir)\n",
    "\n",
    "def save_model(model,model_dir):\n",
    "    path = os.path.join(model_dir,'model.pth')\n",
    "    \n",
    "    torch.save(model.state_dict(),path)\n",
    "    logger.info(f'Saving Model: {path} \\n')\n",
    "\n",
    "def test(model,test_loader, device):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "    tmp_eval_accuracy, eval_accuracy = 0,0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            b_input_ids = batch['input_ids'].to(device)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfb5eb4-6e22-4634-97b2-17715943abf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
